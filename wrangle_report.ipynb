{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRANGLING REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  This report briefly summarizes the efforts that went through to successfully wrangle the WeRateDogs dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this whole project was to be able to test the WeRateDogs dataset to aid in data wrangling. \n",
    "First of all I downloaded the required packages needed to successfully wrangle the dataset.\n",
    "This includes but not limited to numpy, pandas, matplotlib, just to mention a few.\n",
    " \n",
    "I proceeded to the data gathering stage where I downloaded the WeRateDogs Twitter archive dataset. I used a request package to download the tweet image prediction file from the internet with the URL provided. This exposed me to other forms of data gathering techniques which was very enlightening.I queried the Twitter API for each tweet in the Twitter archive and saved the JSON in a text file. I read the tweet json text file into dictionaries and converted the list into a dataframe.\n",
    "\n",
    "I proceeded to the assessment stage where I assessed the data visually and programmatically for all three datasets, namely, Twitter archive enhanced dataset, image prediction dataset and tweet json dataset. Both assessments helped throw a meaningful insight on the dataset which aided in helping observe the quality and tidiness issues within the three datasets.\n",
    "\n",
    "At the end of the visual and programmatic assessments I was able to observe a couple of quality and tidiness issues that I had to work on for all the three datasets to aid in the wrangling and proper cleaning of the datasets respectively. Removing retweets was one of the fundamentals in data warangling process. I worked on that to enable me focus only on the dog ratings and make my dataset clean and analysis more direct. I noted down all these issues and worked on them systematically to provide more insight on the dataset and make it more clean.\n",
    "\n",
    "I proceeded to the cleaning stage where the task was to make the dataset more cleaner. This is where I worked on resolving and correcting the higlighted quality and tidiness issues raised earlier.\n",
    "\n",
    "Finally I gained insight from the dataset, analyzed and visualized my findings for better clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'wrangle_report.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
